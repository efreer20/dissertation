---
title: "Research Question 1 - 1b 1c 1d 1e"
author: "Emily Bovee"
date: "5/14/2019"
output: html_document
---
# Common Code Bank for All Multivariate Random Forest Models
```{r}
#----------------------Load relevant libraries
library(MultivariateRandomForest)
library(pROC)
library(tidyverse)
library(beepr)
library(doParallel)
library(foreach)
setwd("/Volumes/Linnen Lab/MSU Engineering/PROJECTS/Emily Dissertation/*May 2019 Party*")
getwd()



##Performance: caclulate RMSE with a function

RMSE<- function(pred, obs){
  diff <- pred-obs
  diffsq <- diff^2
  numerator<- sum(diffsq)
  N = length(diff)
  return(sqrt(numerator /N))
}

##################################
#MODEL 1b
##################################
# Common Code Bank for All Multivariate Random Forest Models

#----------------------Load relevant data
  # Use the output of the Merge!.R file: randomforestdata.csv

#using imputation 2 because that was the dataset that I randomly selected to use for this analysis

data <- read_csv("randomforestdata_3.18.19_withURM.csv")

    #Right now we are at 1287 obs. of 64 variables

#-------Delete listwise
#Remove nas from the dataset
RFdata <- na.omit(data)  
  #1044 observations of 64

#----------------------Set seed
set.seed(2020)
#----------------------Set up relevant functions

#------- cv function
  #taken from the documentation for Amelia Package
  #adding the set.seed argument
cvData = function (X, Y, F, seed=2020) {
  DrugNumber = nrow(Y)
  Index = 1:DrugNumber
  FoldedIndex = NULL
  FF = F - 1
  for (i in 1:FF) {
    set.seed(seed)
    FoldedIndex[[i]] = sample(Index, floor(DrugNumber/F))
    Index = setdiff(Index, FoldedIndex[[i]])
    if (i == FF) {
      FoldedIndex[[i + 1]] = Index
    }
  }
  TestingIndex = NULL
  TrainingIndex = NULL
  TrainingData = NULL
  TestingData = NULL
  OutputTrain = NULL
  OutputTest = NULL
  for (i in 1:F) {
    TestingIndex[[i]] = FoldedIndex[[i]]
    TrainingIndex[[i]] = setdiff(1:DrugNumber, TestingIndex[[i]])
    TrainingData[[i]] = X[TrainingIndex[[i]], , drop = FALSE]
    TestingData[[i]] = X[TestingIndex[[i]], , drop = FALSE]
    OutputTrain[[i]] = Y[TrainingIndex[[i]], , drop = FALSE]
    OutputTest[[i]] = Y[TestingIndex[[i]], , drop = FALSE]
  }
  result = list(TrainingData, TestingData, OutputTrain, OutputTest, 
                FoldedIndex)
  return(result)
}

#The GitHub Repository from which this is derived
#https://github.com/cran/MultivariateRandomForest/blob/master/R/CrossValidation.R

```

# Select the variables that are needed for this random forest
Model 1b: Spring â€™16 Predictors
```{r}
colnames(RFdata)

#selecting only the variables that are needed for this specific analysis
RFdata <- RFdata %>% 
                select(c(
                  "FEMALE",
                  "URM",
                  "FIRSTGEN",
                  "Max_ACT_Composite",
                  "mlc_yr1",
                  "V2_ESE",
                  "V2_Val",
                  "V4_ESE",
                  "V4_Val"

                ))


#put(outcome/s) at the front of the dataset
RFdata <- RFdata %>% 
  select(c(V4_ESE, V4_Val), everything())


```
# Random Forest

```{r}
#------------
#Generate matrices from dataset in order to later do cross-validation
#------------
#The arguments I need for cross-validation
        #X	      M x N Input matrix, M is the number of samples and N is the number of features
        #Y	      output responses as column vector
        #F	      Number of Folds
#---------- Create X and Y
              # X is my data frame in a matrix format with just the features / covariates for the 
                  #model
              # Y is my data frame again in a matrix format with just the dependent variables

#Creating the x input matrix (predictors)
  #Make a matrix of predictor variables, for training X, exclude outcome
 X = data.matrix (RFdata [, -(1:2)])
 
#Creating the Y input matrix (outcome)
  #Make a matrix for training Y - outcome variables only
  Y = data.matrix (RFdata [, (1:2)])
  
###--------------------------------- 
#Further Reading
###---------------------------------  
#More about creating a matrix from a dataframe
      #https://stat.ethz.ch/R-manual/R-devel/library/base/html/data.matrix.html

# More about subsetting matrices
# http://astrostatistics.psu.edu/su07/R/html/base/html/subset.html

###---------------------------------  
#Notes
###---------------------------------  
  ###NOTE TO SELF: Make sure data is in the correct order because this calls on you to ignore certain variables and it's important to know the order of the variables in order for this to make sense
                #for example: the X matrix has -(1:2), meaning we want to exclude the first two rows
                #the second matrix has 1, 
```


```{r}
# generate dataset for cross-validation
crossData <- cvData(X, Y, F=5, seed=3)
xTrainList = crossData[[1]]
xTestList = crossData[[2]]
yTrainList = crossData[[3]]
yTestList = crossData[[4]]

```

```{r}
#-------parameters for the random forest
n_tree=5
m_feature=5
min_leaf=5

#-------For Loop to run the random forest itself
preds = NULL

cl <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cl)

for(ii in 1:length(xTrainList)){
  # organize data to run on one fold of cross-validated data
  trainX = xTrainList[[ ii ]]
  trainY = yTrainList[[ ii ]]
  testX = xTestList[[ ii ]]
  testY = yTestList[[ii]]

  # generate predictions for that fold
  foldPreds = build_forest_predict(
    trainX, trainY, n_tree, m_feature, min_leaf, testX
    )  
  
  # organize into big preds df
  colnames(foldPreds) = paste0('yhat_',1:ncol(foldPreds))
  colnames(testY) = paste0('y_',1:ncol(testY))
  foldPreds = cbind(foldPreds, testY)
  preds = rbind(preds, foldPreds)
}

stopCluster(cl)
registerDoSEQ()

dim(preds)
head(preds)

## Rename the preds df to correspond to this specific one that I'm on

model_1b <- preds

#New RMSE Evaluation 4.4.19
#UNSTANDARDIZED RMSE
#####first doing the unstandardized

#Bind the data together: Observed for ESE and Val
pred_1b <- rbind(model_1b[1], model_1b[2])
obs_1b <- rbind(model_1b[3], model_1b[4])

RMSE_1b <- RMSE(pred_1b, obs_1b)

#STANDARDIZED RMSE
#####next, trying standardized version

#First, writing a function to standardize the predicted and actual
  stdz = function(x){ (x - mean(x))/sd(x) } 

#STANDARDIZE
pred_1b <- stdz(pred_1b)
obs_1b <- stdz(obs_1b)

#Get Standardized RMSEs
RMSE_1b_stdz <- RMSE(pred_1b, obs_1b)
```

##################################
MODEL 1c
##################################

# Common Code Bank for All Multivariate Random Forest Models
```{r}
#----------------------Load relevant libraries
#----------------------Load relevant data
  # Use the output of the Merge!.R file: randomforestdata.csv

#using imputation 2 because that was the dataset that I randomly selected to use for this analysis

data <- read_csv("randomforestdata_3.18.19_withURM.csv")


#-------Delete listwise
#Remove nas from the dataset
RFdata <- na.omit(data)  
  #1044 observations of 64

#----------------------Set seed
set.seed(2020)
#----------------------Set up relevant functions

#------- cv function
  #taken from the documentation for Amelia Package
  #adding the set.seed argument
cvData = function (X, Y, F, seed=2020) {
  DrugNumber = nrow(Y)
  Index = 1:DrugNumber
  FoldedIndex = NULL
  FF = F - 1
  for (i in 1:FF) {
    set.seed(seed)
    FoldedIndex[[i]] = sample(Index, floor(DrugNumber/F))
    Index = setdiff(Index, FoldedIndex[[i]])
    if (i == FF) {
      FoldedIndex[[i + 1]] = Index
    }
  }
  TestingIndex = NULL
  TrainingIndex = NULL
  TrainingData = NULL
  TestingData = NULL
  OutputTrain = NULL
  OutputTest = NULL
  for (i in 1:F) {
    TestingIndex[[i]] = FoldedIndex[[i]]
    TrainingIndex[[i]] = setdiff(1:DrugNumber, TestingIndex[[i]])
    TrainingData[[i]] = X[TrainingIndex[[i]], , drop = FALSE]
    TestingData[[i]] = X[TestingIndex[[i]], , drop = FALSE]
    OutputTrain[[i]] = Y[TrainingIndex[[i]], , drop = FALSE]
    OutputTest[[i]] = Y[TestingIndex[[i]], , drop = FALSE]
  }
  result = list(TrainingData, TestingData, OutputTrain, OutputTest, 
                FoldedIndex)
  return(result)
}

#The GitHub Repository from which this is derived
#https://github.com/cran/MultivariateRandomForest/blob/master/R/CrossValidation.R

```
Predictors:
  Gender
  Race
  FirstGen
  ACT score

MLC Summer 16
MLC Fall 16
MLC Spring 17

Center Summer 16
Center Fall 16
Center Spring 17
 
Adv Summer 16
Adv Fall 16
Adv Spring 17

Expectancy Spring 2017 (V3)
Value Spring 2017 (V3)
  
Outcomes: 
  Expectancy V4
  Value V4

```{r}
colnames(RFdata)

#selecting only the variables that are needed for this specific analysis
RFdata <- RFdata %>% 
                select(c(
                  "FEMALE",
                  "URM",
                  "FIRSTGEN",
                  "Max_ACT_Composite",   
	                "center_event_yr2", 
                  "center_adv_yr2", 
                  "mock_yr2",
                  "mlc_yr2",
                  "adv_yr2",    
                  "V3_ESE",
                  "V3_Val",
                  "V4_ESE",
                  "V4_Val"
                ))

#put(outcome/s) at the front of the dataset
RFdata <- RFdata %>% 
  select(c(V4_ESE, V4_Val), everything())

```

# Random Forest

```{r}
#------------
#Generate matrices from dataset in order to later do cross-validation
#------------
#The arguments I need for cross-validation
        #X	      M x N Input matrix, M is the number of samples and N is the number of features
        #Y	      output responses as column vector
        #F	      Number of Folds


#---------- Create X and Y
              # X is my data frame in a matrix format with just the features / covariates for the 
                  #model
              # Y is my data frame again in a matrix format with just the dependent variables


#Creating the x input matrix (predictors)
  #Make a matrix of predictor variables, for training X, exclude outcome
 X = data.matrix (RFdata [, -(1:2)])
 
#Creating the Y input matrix (outcome)
  #Make a matrix for training Y - outcome variables only
  Y = data.matrix (RFdata [, (1:2)])
  
###--------------------------------- 
#Further Reading
###---------------------------------  
#More about creating a matrix from a dataframe
      #https://stat.ethz.ch/R-manual/R-devel/library/base/html/data.matrix.html

# More about subsetting matrices
# http://astrostatistics.psu.edu/su07/R/html/base/html/subset.html

###---------------------------------  
#Notes
###---------------------------------  
  ###NOTE TO SELF: Make sure data is in the correct order because this calls on you to ignore certain variables and it's important to know the order of the variables in order for this to make sense
                #for example: the X matrix has -(1:2), meaning we want to exclude the first two rows
                #the second matrix has 1, 
```


```{r}

# generate dataset for cross-validation

crossData <- cvData(X, Y, F=5, seed=3)
xTrainList = crossData[[1]]
xTestList = crossData[[2]]
yTrainList = crossData[[3]]
yTestList = crossData[[4]]

```

```{r}
#-------parameters for the random forest
n_tree=5
m_feature=5
min_leaf=5

#-------For Loop to run the random forest itself
preds = NULL

cl <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cl)

for(ii in 1:length(xTrainList)){
  # organize data to run on one fold of cross-validated data
  trainX = xTrainList[[ ii ]]
  trainY = yTrainList[[ ii ]]
  testX = xTestList[[ ii ]]
  testY = yTestList[[ii]]

  # generate predictions for that fold
  foldPreds = build_forest_predict(
    trainX, trainY, n_tree, m_feature, min_leaf, testX
    )  
  
  # organize into big preds df
  colnames(foldPreds) = paste0('yhat_',1:ncol(foldPreds))
  colnames(testY) = paste0('y_',1:ncol(testY))
  foldPreds = cbind(foldPreds, testY)
  preds = rbind(preds, foldPreds)
}

stopCluster(cl)
registerDoSEQ()

dim(preds)
head(preds)

## Rename the preds df to correspond to this specific one that I'm on

model_1c <- preds


#New RMSE Evaluation 4.4.19
#UNSTANDARDIZED RMSE
#####first doing the unstandardized

#Bind the data together: Observed for ESE and Val
pred_1c <- rbind(model_1c[1], model_1c[2])
obs_1c <- rbind(model_1c[3], model_1c[4])

RMSE_1c <- RMSE(pred_1c, obs_1c)

#STANDARDIZED RMSE
#####next, trying standardized version

#STANDARDIZE
pred_1c <- stdz(pred_1c)
obs_1c <- stdz(obs_1c)

#Get Standardized RMSEs
RMSE_1c_stdz <- RMSE(pred_1c, obs_1c)
```



##################################
MODEL 1d
##################################

# Common Code Bank for All Multivariate Random Forest Models
```{r}
#----------------------Load relevant libraries

#----------------------Load relevant data
  # Use the output of the Merge!.R file: randomforestdata.csv

#using imputation 2 because that was the dataset that I randomly selected to use for this analysis

data <- read_csv("randomforestdata_3.18.19_withURM.csv")

#-------Delete listwise
#Remove nas from the dataset
RFdata <- na.omit(data)  

#----------------------Set seed
set.seed(2020)
#----------------------Set up relevant functions

#------- cv function
  #taken from the documentation for Amelia Package
  #adding the set.seed argument
cvData = function (X, Y, F, seed=2020) {
  DrugNumber = nrow(Y)
  Index = 1:DrugNumber
  FoldedIndex = NULL
  FF = F - 1
  for (i in 1:FF) {
    set.seed(seed)
    FoldedIndex[[i]] = sample(Index, floor(DrugNumber/F))
    Index = setdiff(Index, FoldedIndex[[i]])
    if (i == FF) {
      FoldedIndex[[i + 1]] = Index
    }
  }
  TestingIndex = NULL
  TrainingIndex = NULL
  TrainingData = NULL
  TestingData = NULL
  OutputTrain = NULL
  OutputTest = NULL
  for (i in 1:F) {
    TestingIndex[[i]] = FoldedIndex[[i]]
    TrainingIndex[[i]] = setdiff(1:DrugNumber, TestingIndex[[i]])
    TrainingData[[i]] = X[TrainingIndex[[i]], , drop = FALSE]
    TestingData[[i]] = X[TestingIndex[[i]], , drop = FALSE]
    OutputTrain[[i]] = Y[TrainingIndex[[i]], , drop = FALSE]
    OutputTest[[i]] = Y[TestingIndex[[i]], , drop = FALSE]
  }
  result = list(TrainingData, TestingData, OutputTrain, OutputTest, 
                FoldedIndex)
  return(result)
}

#The GitHub Repository from which this is derived
#https://github.com/cran/MultivariateRandomForest/blob/master/R/CrossValidation.R

```
```{r}
colnames(RFdata)

#selecting only the variables that are needed for this specific analysis
RFdata <- RFdata %>% 
                select(c(
"FEMALE",
                  "URM",
                  "FIRSTGEN",
                  "Max_ACT_Composite",   
	     "center_event_yr3", 
     "center_adv_yr3", 
    "mock_yr3",
     "mlc_yr3",
    "adv_yr3",    
                  "V3_ESE",
                  "V3_Val",
                  "V4_ESE",
                  "V4_Val"
 ))


#put(outcome/s) at the front of the dataset
RFdata <- RFdata %>% 
  select(c(V4_ESE, V4_Val), everything())

```

# Random Forest

```{r}
#------------
#Generate matrices from dataset in order to later do cross-validation
#------------
#The arguments I need for cross-validation
        #X	      M x N Input matrix, M is the number of samples and N is the number of features
        #Y	      output responses as column vector
        #F	      Number of Folds


#---------- Create X and Y
              # X is my data frame in a matrix format with just the features / covariates for the 
                  #model
              # Y is my data frame again in a matrix format with just the dependent variables


#Creating the x input matrix (predictors)
  #Make a matrix of predictor variables, for training X, exclude outcome
 X = data.matrix (RFdata [, -(1:2)])
 
#Creating the Y input matrix (outcome)
  #Make a matrix for training Y - outcome variables only
  Y = data.matrix (RFdata [, (1:2)])
  
###--------------------------------- 
#Further Reading
###---------------------------------  
#More about creating a matrix from a dataframe
      #https://stat.ethz.ch/R-manual/R-devel/library/base/html/data.matrix.html

# More about subsetting matrices
# http://astrostatistics.psu.edu/su07/R/html/base/html/subset.html

###---------------------------------  
#Notes
###---------------------------------  
  ###NOTE TO SELF: Make sure data is in the correct order because this calls on you to ignore certain variables and it's important to know the order of the variables in order for this to make sense
                #for example: the X matrix has -(1:2), meaning we want to exclude the first two rows
                #the second matrix has 1, 
```


```{r}

# generate dataset for cross-validation

crossData <- cvData(X, Y, F=5, seed=3)
xTrainList = crossData[[1]]
xTestList = crossData[[2]]
yTrainList = crossData[[3]]
yTestList = crossData[[4]]

```

```{r}
#-------parameters for the random forest
n_tree=5
m_feature=5
min_leaf=5

#-------For Loop to run the random forest itself
preds = NULL


cl=makeCluster(6) # set number of cores to use
registerDoParallel(cl) # pass them off to r session
preds = foreach(
  ii = 1:length(xTrainList), # loop through # of folds indexing by i
  .packages=c('MultivariateRandomForest') # load necessary packages
  ) %dopar% { # start of parallelization loop

  # organize data to run on one fold of cross-validated data
  trainX = xTrainList[[ ii ]]
  trainY = yTrainList[[ ii ]]
  testX = xTestList[[ ii ]]
  testY = yTestList[[ii]]

  # generate predictions for that fold
  foldPreds = build_forest_predict(
    trainX, trainY, n_tree, m_feature, min_leaf, testX
    )  
  
  # organize into big preds df
  colnames(foldPreds) = paste0('yhat_',1:ncol(foldPreds))
  colnames(testY) = paste0('y_',1:ncol(testY))
  foldPreds = cbind(foldPreds, testY)
  return(foldPreds)
} # end of parallelization loop

stopCluster(cl) # close extra cores

# organize results into dataframe
preds = do.call('rbind', preds)


dim(preds)
head(preds)

## Rename the preds df to correspond to this specific one that I'm on

model_1d <- preds


#New RMSE Evaluation 4.4.19
#UNSTANDARDIZED RMSE
#####first doing the unstandardized

#Bind the data together: Observed for ESE and Val
pred_1d <- rbind(model_1d[1], model_1d[2])
obs_1d <- rbind(model_1d[3], model_1d[4])

RMSE_1d <- RMSE(pred_1d, obs_1d)

#STANDARDIZED RMSE
#####next, trying standardized version

#STANDARDIZE
pred_1d <- stdz(pred_1d)
obs_1d <- stdz(obs_1d)

#Get Standardized RMSEs
RMSE_1d_stdz <- RMSE(pred_1d, obs_1d)

```


##################################
MODEL 1e
##################################


#NOTE: Ran this one on the Linnenlab computer so working directory is different

# Common Code Bank for All Multivariate Random Forest Models
```{r}
#----------------------Load relevant libraries

#----------------------Load relevant data
  # Use the output of the Merge!.R file: randomforestdata.csv

#using imputation 2 because that was the dataset that I randomly selected to use for this analysis

data <- read_csv("randomforestdata_3.18.19_withURM.csv")

    #Right now we are at 1287 obs. of 64 variables

#-------Delete listwise
#Remove nas from the dataset
RFdata <- na.omit(data)  
  #1044 observations of 64

#----------------------Set seed
set.seed(2020)
#----------------------Set up relevant functions

#------- cv function
  #taken from the documentation for Amelia Package
  #adding the set.seed argument
cvData = function (X, Y, F, seed=2020) {
  DrugNumber = nrow(Y)
  Index = 1:DrugNumber
  FoldedIndex = NULL
  FF = F - 1
  for (i in 1:FF) {
    set.seed(seed)
    FoldedIndex[[i]] = sample(Index, floor(DrugNumber/F))
    Index = setdiff(Index, FoldedIndex[[i]])
    if (i == FF) {
      FoldedIndex[[i + 1]] = Index
    }
  }
  TestingIndex = NULL
  TrainingIndex = NULL
  TrainingData = NULL
  TestingData = NULL
  OutputTrain = NULL
  OutputTest = NULL
  for (i in 1:F) {
    TestingIndex[[i]] = FoldedIndex[[i]]
    TrainingIndex[[i]] = setdiff(1:DrugNumber, TestingIndex[[i]])
    TrainingData[[i]] = X[TrainingIndex[[i]], , drop = FALSE]
    TestingData[[i]] = X[TestingIndex[[i]], , drop = FALSE]
    OutputTrain[[i]] = Y[TrainingIndex[[i]], , drop = FALSE]
    OutputTest[[i]] = Y[TestingIndex[[i]], , drop = FALSE]
  }
  result = list(TrainingData, TestingData, OutputTrain, OutputTest, 
                FoldedIndex)
  return(result)
}

#The GitHub Repository from which this is derived
#https://github.com/cran/MultivariateRandomForest/blob/master/R/CrossValidation.R

```
```{r}
colnames(RFdata)

#selecting only the variables that are needed for this specific analysis
RFdata <- RFdata %>% 
  select(c(
          #demographics
        "FEMALE",
        "URM",
        "FIRSTGEN",
        "Max_ACT_Composite",   
        "SouthcomplexBinary_2015",
        #math learning
        "mlc_yr1",
        "mlc_yr2",
        "mlc_yr3", 
        #Center
        "center_event_yr2", 
        "center_adv_yr2", 
        "mock_yr2",
        "center_event_yr3", 
        "center_adv_yr3", 
        "mock_yr3",
        "adv_yr2",    
        "adv_yr3",    
        #Motivation, all 4 time points
                  "V1_ESE",
                  "V1_Val",
                  "V2_ESE",
                  "V2_Val",
                  "V3_ESE",
                  "V3_Val",
                  "V4_ESE",
                  "V4_Val"))

#put(outcome/s) at the front of the dataset
RFdata <- RFdata %>% 
  select(c(V4_ESE, V4_Val), everything())


```

# Random Forest

```{r}
#------------
#Generate matrices from dataset in order to later do cross-validation
#------------
#The arguments I need for cross-validation
        #X	      M x N Input matrix, M is the number of samples and N is the number of features
        #Y	      output responses as column vector
        #F	      Number of Folds


#---------- Create X and Y
              # X is my data frame in a matrix format with just the features / covariates for the 
                  #model
              # Y is my data frame again in a matrix format with just the dependent variables


#Creating the x input matrix (predictors)
  #Make a matrix of predictor variables, for training X, exclude outcome
 X = data.matrix (RFdata [, -(1:2)])
 
#Creating the Y input matrix (outcome)
  #Make a matrix for training Y - outcome variables only
  Y = data.matrix (RFdata [, (1:2)])
  
###--------------------------------- 
#Further Reading
###---------------------------------  
#More about creating a matrix from a dataframe
      #https://stat.ethz.ch/R-manual/R-devel/library/base/html/data.matrix.html

# More about subsetting matrices
# http://astrostatistics.psu.edu/su07/R/html/base/html/subset.html

###---------------------------------  
#Notes
###---------------------------------  
  ###NOTE TO SELF: Make sure data is in the correct order because this calls on you to ignore certain variables and it's important to know the order of the variables in order for this to make sense
                #for example: the X matrix has -(1:2), meaning we want to exclude the first two rows
                #the second matrix has 1, 
```


```{r}

# generate dataset for cross-validation

crossData <- cvData(X, Y, F=5, seed=3)
xTrainList = crossData[[1]]
xTestList = crossData[[2]]
yTrainList = crossData[[3]]
yTestList = crossData[[4]]

```

```{r}
#-------parameters for the random forest
n_tree=5
m_feature=5
min_leaf=5

#-------For Loop to run the random forest itself
preds = NULL


cl=makeCluster(6) # set number of cores to use
registerDoParallel(cl) # pass them off to r session
preds = foreach(
  ii = 1:length(xTrainList), # loop through # of folds indexing by i
  .packages=c('MultivariateRandomForest') # load necessary packages
  ) %dopar% { # start of parallelization loop

  # organize data to run on one fold of cross-validated data
  trainX = xTrainList[[ ii ]]
  trainY = yTrainList[[ ii ]]
  testX = xTestList[[ ii ]]
  testY = yTestList[[ii]]

  # generate predictions for that fold
  foldPreds = build_forest_predict(
    trainX, trainY, n_tree, m_feature, min_leaf, testX
    )  
  
  # organize into big preds df
  colnames(foldPreds) = paste0('yhat_',1:ncol(foldPreds))
  colnames(testY) = paste0('y_',1:ncol(testY))
  foldPreds = cbind(foldPreds, testY)
  return(foldPreds)
} # end of parallelization loop

stopCluster(cl) # close extra cores

# organize results into dataframe
preds = do.call('rbind', preds)


dim(preds)
head(preds)

## Rename the preds df to correspond to this specific one that I'm on

model_1e <- preds

#New RMSE Evaluation 4.4.19
#UNSTANDARDIZED RMSE
#####first doing the unstandardized

#Bind the data together: Observed for ESE and Val
pred_1e <- rbind(model_1e[1], model_1e[2])
obs_1e <- rbind(model_1e[3], model_1e[4])

RMSE_1e <- RMSE(pred_1e, obs_1e)

#STANDARDIZED RMSE
#####next, trying standardized version

#STANDARDIZE
pred_1e <- stdz(pred_1e)
obs_1e <- stdz(obs_1e)

#Get Standardized RMSEs
RMSE_1e_stdz <- RMSE(pred_1e, obs_1e)

##Print all RMSEs

RMSE_1b
RMSE_1b_stdz

RMSE_1c
RMSE_1c_stdz

RMSE_1d
RMSE_1d_stdz

RMSE_1e
RMSE_1e_stdz


###

#Save predicted/actual datasets in case I need to do different RMSE calculations in the future
saveRDS(model_1b,file="model_1b.Rda")
saveRDS(model_1c,file="model_1c.Rda")
saveRDS(model_1d,file="model_1d.Rda")
saveRDS(model_1e,file="model_1e.Rda")

#Write CSVs for predicted/actual in case I want to do the calculations offline


model_1b <- as.data.frame(model_1b)
model_1c <- as.data.frame(model_1c)
model_1d <- as.data.frame(model_1d)
model_1e <- as.data.frame(model_1e)


write_csv(model_1b, "model_1b.csv")
write_csv(model_1c, "model_1c.csv")
write_csv(model_1d, "model_1d.csv")
write_csv(model_1e, "model_1e.csv")
```


#This variance explained code is not evaluated yet, but I thought it would be good to get my outputs saved in a smart place
